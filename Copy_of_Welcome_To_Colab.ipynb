{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarparmar072000-bit/sagar/blob/main/Copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, GlobalAveragePooling1D\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Data Loading (Placeholder) ---\n",
        "# NOTE: Replace this with your actual data loading code for the Enron-Phish dataset.\n",
        "# Example: df = pd.read_csv('your_enron_phish_dataset.csv')\n",
        "# For demonstration, we will create a simple mock dataset.\n",
        "print(\"Loading dataset...\")\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Congratulations! You've won a free prize. Click here now!\",\n",
        "        \"Phishing scam alert: Do not click this link.\",\n",
        "        \"Your account has been suspended. Click this link to verify.\",\n",
        "        \"Meeting at 2pm tomorrow. Please see the attached agenda.\",\n",
        "        \"Please update your payment information to avoid service interruption.\",\n",
        "        \"Regarding the project report for next week, let me know your thoughts.\",\n",
        "        \"Please login to your bank account via the link below.\",\n",
        "        \"Hello, just checking in. Hope you are well.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0] # 1 for phishing, 0 for legitimate\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# --- Pre-processing ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses text data.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    return text\n",
        "\n",
        "print(\"Pre-processing text data...\")\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "max_len = max(len(s) for s in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model Building: LSTM with Attention ---\n",
        "print(\"Building LSTM with Attention model...\")\n",
        "vocab_size = 5000\n",
        "embedding_dim = 128\n",
        "lstm_units = 64\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(max_len,))\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)\n",
        "\n",
        "# LSTM layer with return_sequences=True\n",
        "lstm_out = LSTM(lstm_units, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# Attention layer - applied to the LSTM output\n",
        "attention_output = Attention()([lstm_out, lstm_out])\n",
        "\n",
        "# GlobalAveragePooling1D to get a single output per sequence after attention\n",
        "pooled_output = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(pooled_output)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- Model Training ---\n",
        "print(\"Training the model...\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2, verbose=0)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"Evaluating model performance on the test set...\")\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n--- Model Performance Metrics ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR9V4Zw-GXsi",
        "outputId": "c8bdb841-fc20-4583-8446-d9d99656377e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded successfully.\n",
            "Pre-processing text data...\n",
            "Building LSTM with Attention model...\n",
            "Training the model...\n",
            "Model training complete.\n",
            "Evaluating model performance on the test set...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
            "\n",
            "--- Model Performance Metrics ---\n",
            "Accuracy: 0.3333\n",
            "Precision: 0.1111\n",
            "Recall: 0.3333\n",
            "F1-Score: 0.1667\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}